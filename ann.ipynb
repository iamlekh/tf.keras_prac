{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_excel('Credit Card _Dataset.xls')\n",
    "df.head()\n",
    "X = df.iloc[:,1:24]\n",
    "Y = df.iloc[:,24]\n",
    "X.set_index(np.arange(30000), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "30000/30000 [==============================] - 11s 382us/step - loss: 3.5653 - acc: 0.7788\n",
      "Epoch 2/10\n",
      "30000/30000 [==============================] - 11s 359us/step - loss: 3.5653 - acc: 0.7788\n",
      "Epoch 3/10\n",
      "30000/30000 [==============================] - 10s 333us/step - loss: 3.5653 - acc: 0.7788\n",
      "Epoch 4/10\n",
      "30000/30000 [==============================] - 11s 350us/step - loss: 3.5653 - acc: 0.7788\n",
      "Epoch 5/10\n",
      "30000/30000 [==============================] - 11s 355us/step - loss: 3.5653 - acc: 0.7788\n",
      "Epoch 6/10\n",
      "30000/30000 [==============================] - 10s 348us/step - loss: 3.5653 - acc: 0.7788\n",
      "Epoch 7/10\n",
      "30000/30000 [==============================] - 10s 327us/step - loss: 3.5653 - acc: 0.7788\n",
      "Epoch 8/10\n",
      "30000/30000 [==============================] - 9s 300us/step - loss: 3.5653 - acc: 0.7788\n",
      "Epoch 9/10\n",
      "30000/30000 [==============================] - 11s 359us/step - loss: 3.5653 - acc: 0.7788\n",
      "Epoch 10/10\n",
      "30000/30000 [==============================] - 10s 328us/step - loss: 3.5653 - acc: 0.7788\n",
      "30000/30000 [==============================] - 2s 51us/step\n",
      "\n",
      "acc: 77.88%\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "X = df.iloc[:,1:24]\n",
    "Y = df.iloc[:,24]\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=23, activation= 'relu' ))\n",
    "model.add(Dense(8, activation= 'relu' ))\n",
    "model.add(Dense(1, activation= 'sigmoid' ))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, Y, epochs=3, batch_size=10)\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X, Y)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use a Automatic Verification Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20099 samples, validate on 9901 samples\n",
      "Epoch 1/10\n",
      "20099/20099 [==============================] - 8s 411us/step - loss: 3.6657 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 2/10\n",
      "20099/20099 [==============================] - 7s 327us/step - loss: 3.6657 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 3/10\n",
      "20099/20099 [==============================] - 7s 371us/step - loss: 3.6657 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 4/10\n",
      "20099/20099 [==============================] - 7s 368us/step - loss: 3.6657 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 5/10\n",
      "20099/20099 [==============================] - 8s 391us/step - loss: 3.6657 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 6/10\n",
      "20099/20099 [==============================] - 9s 444us/step - loss: 3.6657 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 7/10\n",
      "20099/20099 [==============================] - 7s 355us/step - loss: 3.6657 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 8/10\n",
      "20099/20099 [==============================] - 8s 388us/step - loss: 3.6657 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 9/10\n",
      "20099/20099 [==============================] - 7s 364us/step - loss: 3.6657 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 10/10\n",
      "20099/20099 [==============================] - 8s 418us/step - loss: 3.6657 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fda383c2b38>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLP with automatic validation set\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = df.iloc[:,1:24]\n",
    "Y = df.iloc[:,24]\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=23, activation= 'relu' ))\n",
    "model.add(Dense(8, activation= 'relu' ))\n",
    "model.add(Dense(1, activation= 'sigmoid' ))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, Y, validation_split=0.33, epochs=3, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use a Manual Verification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20100 samples, validate on 9900 samples\n",
      "Epoch 1/5\n",
      "20100/20100 [==============================] - 8s 408us/step - loss: 3.5797 - acc: 0.7779 - val_loss: 3.5362 - val_acc: 0.7806\n",
      "Epoch 2/5\n",
      "20100/20100 [==============================] - 8s 386us/step - loss: 3.5797 - acc: 0.7779 - val_loss: 3.5362 - val_acc: 0.7806\n",
      "Epoch 3/5\n",
      "20100/20100 [==============================] - 8s 408us/step - loss: 3.5797 - acc: 0.7779 - val_loss: 3.5362 - val_acc: 0.7806\n",
      "Epoch 4/5\n",
      "20100/20100 [==============================] - 8s 401us/step - loss: 3.5797 - acc: 0.7779 - val_loss: 3.5362 - val_acc: 0.7806\n",
      "Epoch 5/5\n",
      "20100/20100 [==============================] - 9s 442us/step - loss: 3.5797 - acc: 0.7779 - val_loss: 3.5362 - val_acc: 0.7806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd9f0c46fd0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLP with manual validation set\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = df.iloc[:,1:24]\n",
    "Y = df.iloc[:,24]\n",
    "# split into 67% for train and 33% for test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=seed)\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=23, activation= 'relu' ))\n",
    "model.add(Dense(8, activation= 'relu' ))\n",
    "model.add(Dense(1, activation= 'sigmoid' ))\n",
    "# Compile model\n",
    "model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=5, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual k-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3001/3001 [==============================] - 0s 150us/step\n",
      "acc: 77.87%\n",
      "3001/3001 [==============================] - 0s 160us/step\n",
      "acc: 77.87%\n",
      "3001/3001 [==============================] - 0s 163us/step\n",
      "acc: 22.23%\n",
      "3001/3001 [==============================] - 1s 171us/step\n",
      "acc: 22.73%\n",
      "3000/3000 [==============================] - 1s 178us/step\n",
      "acc: 77.87%\n",
      "3000/3000 [==============================] - 1s 182us/step\n",
      "acc: 22.13%\n",
      "2999/2999 [==============================] - 1s 218us/step\n",
      "acc: 77.89%\n",
      "2999/2999 [==============================] - 1s 198us/step\n",
      "acc: 77.89%\n",
      "2999/2999 [==============================] - 1s 202us/step\n",
      "acc: 22.17%\n",
      "2999/2999 [==============================] - 1s 217us/step\n",
      "acc: 77.89%\n",
      "55.66% (+/- 27.22%)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = df.iloc[:,1:24]\n",
    "Y = df.iloc[:,24]\n",
    "\n",
    "# define 10-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "kfold.get_n_splits(X,Y)\n",
    "cvscores = []\n",
    "\n",
    "for train, test in kfold.split(X, Y):\n",
    "    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=23, activation= 'relu' ))\n",
    "    model.add(Dense(8, activation= 'relu' ))\n",
    "    model.add(Dense(1, activation= 'sigmoid' ))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X.iloc[train], Y.iloc[train], epochs=2, batch_size=10, verbose=0)\n",
    "    \n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(X.iloc[test], Y.iloc[test], verbose=1)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate A Neural Network Using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/5\n",
      "26999/26999 [==============================] - 9s 318us/step - loss: 3.5652 - acc: 0.7788\n",
      "Epoch 2/5\n",
      "26999/26999 [==============================] - 7s 272us/step - loss: 3.5652 - acc: 0.7788\n",
      "Epoch 3/5\n",
      "26999/26999 [==============================] - 8s 281us/step - loss: 3.5652 - acc: 0.7788\n",
      "Epoch 4/5\n",
      "26999/26999 [==============================] - 7s 271us/step - loss: 3.5652 - acc: 0.7788\n",
      "Epoch 5/5\n",
      "26999/26999 [==============================] - 8s 298us/step - loss: 3.5652 - acc: 0.7788\n",
      "3001/3001 [==============================] - 0s 127us/step\n",
      "Epoch 1/5\n",
      "26999/26999 [==============================] - 10s 374us/step - loss: 3.7264 - acc: 0.7687\n",
      "Epoch 2/5\n",
      "26999/26999 [==============================] - 9s 335us/step - loss: 3.5658 - acc: 0.7788\n",
      "Epoch 3/5\n",
      "26999/26999 [==============================] - 9s 320us/step - loss: 3.5658 - acc: 0.7788\n",
      "Epoch 4/5\n",
      "26999/26999 [==============================] - 9s 318us/step - loss: 3.5658 - acc: 0.7788\n",
      "Epoch 5/5\n",
      "26999/26999 [==============================] - 8s 308us/step - loss: 3.5658 - acc: 0.7788\n",
      "3001/3001 [==============================] - 0s 142us/step\n",
      "Epoch 1/5\n",
      "26999/26999 [==============================] - 9s 350us/step - loss: 6.4349 - acc: 0.5979\n",
      "Epoch 2/5\n",
      "26999/26999 [==============================] - 8s 311us/step - loss: 3.5688 - acc: 0.7786\n",
      "Epoch 3/5\n",
      "26999/26999 [==============================] - 9s 335us/step - loss: 3.5735 - acc: 0.7783\n",
      "Epoch 4/5\n",
      "26999/26999 [==============================] - 9s 341us/step - loss: 3.5735 - acc: 0.7783\n",
      "Epoch 5/5\n",
      "26999/26999 [==============================] - 9s 324us/step - loss: 3.5735 - acc: 0.7783\n",
      "3001/3001 [==============================] - 0s 139us/step\n",
      "Epoch 1/5\n",
      "26999/26999 [==============================] - 11s 419us/step - loss: 3.5652 - acc: 0.7788\n",
      "Epoch 2/5\n",
      "26999/26999 [==============================] - 10s 359us/step - loss: 3.5652 - acc: 0.7788\n",
      "Epoch 3/5\n",
      "26999/26999 [==============================] - 10s 368us/step - loss: 3.5652 - acc: 0.7788\n",
      "Epoch 4/5\n",
      "26999/26999 [==============================] - 9s 321us/step - loss: 3.5652 - acc: 0.7788\n",
      "Epoch 5/5\n",
      "26999/26999 [==============================] - 9s 325us/step - loss: 3.5652 - acc: 0.7788\n",
      "3001/3001 [==============================] - 0s 143us/step\n",
      "Epoch 1/5\n",
      "27000/27000 [==============================] - 11s 392us/step - loss: 3.8355 - acc: 0.7617\n",
      "Epoch 2/5\n",
      "27000/27000 [==============================] - 9s 339us/step - loss: 3.5651 - acc: 0.7788\n",
      "Epoch 3/5\n",
      "27000/27000 [==============================] - 10s 367us/step - loss: 3.5651 - acc: 0.7788\n",
      "Epoch 4/5\n",
      "27000/27000 [==============================] - 9s 344us/step - loss: 3.5651 - acc: 0.7788\n",
      "Epoch 5/5\n",
      "27000/27000 [==============================] - 10s 354us/step - loss: 3.5651 - acc: 0.7788\n",
      "3000/3000 [==============================] - 0s 155us/step\n",
      "Epoch 1/5\n",
      "27000/27000 [==============================] - 10s 388us/step - loss: 12.3820 - acc: 0.2233\n",
      "Epoch 2/5\n",
      "27000/27000 [==============================] - 11s 405us/step - loss: 12.3820 - acc: 0.2233\n",
      "Epoch 3/5\n",
      "27000/27000 [==============================] - 10s 361us/step - loss: 12.3820 - acc: 0.2233\n",
      "Epoch 4/5\n",
      "27000/27000 [==============================] - 9s 347us/step - loss: 12.3820 - acc: 0.2233\n",
      "Epoch 5/5\n",
      "27000/27000 [==============================] - 9s 341us/step - loss: 12.3820 - acc: 0.2233\n",
      "3000/3000 [==============================] - 1s 199us/step\n",
      "Epoch 1/5\n",
      "27001/27001 [==============================] - 10s 385us/step - loss: 3.5679 - acc: 0.7786\n",
      "Epoch 2/5\n",
      "27001/27001 [==============================] - 9s 333us/step - loss: 3.5673 - acc: 0.7787\n",
      "Epoch 3/5\n",
      "27001/27001 [==============================] - 9s 318us/step - loss: 3.5673 - acc: 0.7787\n",
      "Epoch 4/5\n",
      "27001/27001 [==============================] - 8s 304us/step - loss: 3.5673 - acc: 0.7787\n",
      "Epoch 5/5\n",
      "27001/27001 [==============================] - 8s 303us/step - loss: 3.5673 - acc: 0.7787\n",
      "2999/2999 [==============================] - 1s 174us/step\n",
      "Epoch 1/5\n",
      "27001/27001 [==============================] - 10s 358us/step - loss: 5.2414 - acc: 0.6731\n",
      "Epoch 2/5\n",
      "27001/27001 [==============================] - 9s 331us/step - loss: 3.5911 - acc: 0.7772\n",
      "Epoch 3/5\n",
      "27001/27001 [==============================] - 9s 326us/step - loss: 3.5673 - acc: 0.7787\n",
      "Epoch 4/5\n",
      "27001/27001 [==============================] - 9s 339us/step - loss: 3.5673 - acc: 0.7787\n",
      "Epoch 5/5\n",
      "27001/27001 [==============================] - 11s 390us/step - loss: 3.5673 - acc: 0.7787\n",
      "2999/2999 [==============================] - 1s 228us/step\n",
      "Epoch 1/5\n",
      "27001/27001 [==============================] - 11s 392us/step - loss: 12.4157 - acc: 0.2212\n",
      "Epoch 2/5\n",
      "27001/27001 [==============================] - 9s 321us/step - loss: 12.4157 - acc: 0.2212\n",
      "Epoch 3/5\n",
      "27001/27001 [==============================] - 9s 337us/step - loss: 12.4157 - acc: 0.2212\n",
      "Epoch 4/5\n",
      "27001/27001 [==============================] - 9s 342us/step - loss: 12.4157 - acc: 0.2212\n",
      "Epoch 5/5\n",
      "27001/27001 [==============================] - 9s 342us/step - loss: 12.4157 - acc: 0.2212\n",
      "2999/2999 [==============================] - 1s 220us/step\n",
      "Epoch 1/5\n",
      "27001/27001 [==============================] - 11s 407us/step - loss: 4.1838 - acc: 0.7398\n",
      "Epoch 2/5\n",
      "27001/27001 [==============================] - 10s 352us/step - loss: 3.5661 - acc: 0.7787\n",
      "Epoch 3/5\n",
      "27001/27001 [==============================] - 9s 332us/step - loss: 3.5661 - acc: 0.7787\n",
      "Epoch 4/5\n",
      "27001/27001 [==============================] - 9s 329us/step - loss: 3.5661 - acc: 0.7787\n",
      "Epoch 5/5\n",
      "27001/27001 [==============================] - 9s 328us/step - loss: 3.5661 - acc: 0.7787\n",
      "2999/2999 [==============================] - 1s 231us/step\n",
      "0.6674147624395311\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy\n",
    "\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=23, activation= 'relu' ))\n",
    "    model.add(Dense(8, activation= 'relu' ))\n",
    "    model.add(Dense(1, activation= 'sigmoid' ))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = df.iloc[:,1:24]\n",
    "Y = df.iloc[:,24]\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=5, batch_size=10, verbose=1)\n",
    "\n",
    "# evaluate using 10-fold cross validation\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search Deep Learning Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 5s 230us/step - loss: 3.5275 - acc: 0.7811\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 2s 85us/step - loss: 3.5275 - acc: 0.7811\n",
      "10000/10000 [==============================] - 1s 150us/step\n",
      "20000/20000 [==============================] - 1s 45us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 5s 246us/step - loss: 12.4821 - acc: 0.2170\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 2s 78us/step - loss: 12.4821 - acc: 0.2170\n",
      "10000/10000 [==============================] - 2s 151us/step\n",
      "20000/20000 [==============================] - 1s 39us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 5s 240us/step - loss: 3.6770 - acc: 0.7718\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 2s 81us/step - loss: 3.6757 - acc: 0.7719\n",
      "10000/10000 [==============================] - 2s 159us/step\n",
      "20000/20000 [==============================] - 1s 46us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 5s 237us/step - loss: 3.7946 - acc: 0.7643\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 1s 75us/step - loss: 3.6978 - acc: 0.7704\n",
      "10000/10000 [==============================] - 2s 152us/step\n",
      "20000/20000 [==============================] - 1s 40us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 5s 226us/step - loss: 12.4853 - acc: 0.2168\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 2s 77us/step - loss: 12.4853 - acc: 0.2168\n",
      "10000/10000 [==============================] - 2s 159us/step\n",
      "20000/20000 [==============================] - 1s 44us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 5s 255us/step - loss: 3.6741 - acc: 0.7720\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 2s 86us/step - loss: 3.6741 - acc: 0.7720\n",
      "10000/10000 [==============================] - 2s 158us/step\n",
      "20000/20000 [==============================] - 1s 42us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 5s 232us/step - loss: 0.6783 - acc: 0.7729\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 2s 80us/step - loss: 0.5267 - acc: 0.7809\n",
      "10000/10000 [==============================] - 2s 164us/step\n",
      "20000/20000 [==============================] - 1s 44us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 5s 259us/step - loss: 2.2305 - acc: 0.7218\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 2s 87us/step - loss: 0.7758 - acc: 0.7385\n",
      "10000/10000 [==============================] - 2s 161us/step\n",
      "20000/20000 [==============================] - 1s 43us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 5s 271us/step - loss: 2.5202 - acc: 0.7183\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 2s 89us/step - loss: 0.6503 - acc: 0.7373\n",
      "10000/10000 [==============================] - 2s 184us/step\n",
      "20000/20000 [==============================] - 1s 44us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 5s 270us/step - loss: 0.9573 - acc: 0.7511\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 2s 102us/step - loss: 0.5425 - acc: 0.7786\n",
      "10000/10000 [==============================] - 2s 205us/step\n",
      "20000/20000 [==============================] - 1s 51us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 7s 335us/step - loss: 0.9265 - acc: 0.7597\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 2s 97us/step - loss: 0.5446 - acc: 0.7800\n",
      "10000/10000 [==============================] - 2s 200us/step\n",
      "20000/20000 [==============================] - 1s 48us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 6s 294us/step - loss: 1.3990 - acc: 0.7147\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 2s 88us/step - loss: 0.5513 - acc: 0.7636\n",
      "10000/10000 [==============================] - 2s 243us/step\n",
      "20000/20000 [==============================] - 1s 64us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 6s 311us/step - loss: 0.5953 - acc: 0.7679\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 2s 87us/step - loss: 0.5147 - acc: 0.7808\n",
      "10000/10000 [==============================] - 2s 191us/step\n",
      "20000/20000 [==============================] - 1s 47us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 6s 286us/step - loss: 0.7230 - acc: 0.7613\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 2s 86us/step - loss: 0.5306 - acc: 0.7821\n",
      "10000/10000 [==============================] - 2s 195us/step\n",
      "20000/20000 [==============================] - 1s 47us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 6s 288us/step - loss: 0.7875 - acc: 0.7423\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 2s 107us/step - loss: 0.5426 - acc: 0.7715\n",
      "10000/10000 [==============================] - 2s 192us/step\n",
      "20000/20000 [==============================] - 1s 48us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 6s 297us/step - loss: 0.6053 - acc: 0.7734\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 2s 86us/step - loss: 0.5180 - acc: 0.7809\n",
      "10000/10000 [==============================] - 2s 190us/step\n",
      "20000/20000 [==============================] - 1s 47us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 6s 286us/step - loss: 0.5930 - acc: 0.7722\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 2s 86us/step - loss: 0.5158 - acc: 0.7831\n",
      "10000/10000 [==============================] - 2s 194us/step\n",
      "20000/20000 [==============================] - 1s 46us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 6s 291us/step - loss: 0.6951 - acc: 0.7591\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 2s 90us/step - loss: 0.5297 - acc: 0.7720\n",
      "10000/10000 [==============================] - 2s 201us/step\n",
      "20000/20000 [==============================] - 1s 47us/step\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 6s 283us/step - loss: 12.3633 - acc: 0.2245\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 2s 86us/step - loss: 12.3267 - acc: 0.2268\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 2s 86us/step - loss: 12.3449 - acc: 0.2257\n",
      "10000/10000 [==============================] - 2s 203us/step\n",
      "20000/20000 [==============================] - 1s 50us/step\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 6s 289us/step - loss: 5.9954 - acc: 0.6255\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 2s 86us/step - loss: 4.5224 - acc: 0.7183\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 3.9458 - acc: 0.7547\n",
      "10000/10000 [==============================] - 2s 207us/step\n",
      "20000/20000 [==============================] - 1s 49us/step\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 6s 291us/step - loss: 5.1336 - acc: 0.6794\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 2s 85us/step - loss: 3.8930 - acc: 0.7581\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 2s 85us/step - loss: 3.7543 - acc: 0.7670\n",
      "10000/10000 [==============================] - 2s 208us/step\n",
      "20000/20000 [==============================] - 1s 50us/step\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 6s 307us/step - loss: 12.3986 - acc: 0.2222\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 2s 89us/step - loss: 12.3598 - acc: 0.2247\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 2s 92us/step - loss: 10.1293 - acc: 0.3650\n",
      "10000/10000 [==============================] - 2s 212us/step\n",
      "20000/20000 [==============================] - 1s 49us/step\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 6s 317us/step - loss: 12.4797 - acc: 0.2172\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 12.4797 - acc: 0.2172\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 12.4797 - acc: 0.2172\n",
      "10000/10000 [==============================] - 2s 219us/step\n",
      "20000/20000 [==============================] - 1s 53us/step\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 6s 318us/step - loss: 7.8650 - acc: 0.5079\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 2s 90us/step - loss: 3.7400 - acc: 0.7679\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 3.7400 - acc: 0.7679\n",
      "10000/10000 [==============================] - 2s 222us/step\n",
      "20000/20000 [==============================] - 1s 51us/step\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 6s 309us/step - loss: 1.3285 - acc: 0.7493\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 2s 90us/step - loss: 0.5220 - acc: 0.7811\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 2s 89us/step - loss: 0.5147 - acc: 0.7811\n",
      "10000/10000 [==============================] - 2s 223us/step\n",
      "20000/20000 [==============================] - 1s 52us/step\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 6s 312us/step - loss: 1.1029 - acc: 0.7466\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 2s 89us/step - loss: 0.5241 - acc: 0.7826\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 2s 92us/step - loss: 0.5181 - acc: 0.7831\n",
      "10000/10000 [==============================] - 2s 228us/step\n",
      "20000/20000 [==============================] - 1s 51us/step\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 6s 318us/step - loss: 0.6503 - acc: 0.7547\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 2s 90us/step - loss: 0.5403 - acc: 0.7719\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 2s 92us/step - loss: 0.5304 - acc: 0.7718\n",
      "10000/10000 [==============================] - 2s 227us/step\n",
      "20000/20000 [==============================] - 1s 50us/step\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 7s 325us/step - loss: 0.7777 - acc: 0.7742\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 2s 92us/step - loss: 0.5257 - acc: 0.7811\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 2s 94us/step - loss: 0.5095 - acc: 0.7810\n",
      "10000/10000 [==============================] - 2s 238us/step\n",
      "20000/20000 [==============================] - 1s 54us/step\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 7s 332us/step - loss: 0.8110 - acc: 0.7664\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 0.5175 - acc: 0.7830\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 0.5102 - acc: 0.7831\n",
      "10000/10000 [==============================] - 2s 233us/step\n",
      "20000/20000 [==============================] - 1s 53us/step\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 7s 354us/step - loss: 1.8401 - acc: 0.7349\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 2s 82us/step - loss: 0.5719 - acc: 0.7592\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 2s 78us/step - loss: 0.5307 - acc: 0.7712\n",
      "10000/10000 [==============================] - 2s 219us/step\n",
      "20000/20000 [==============================] - 1s 59us/step\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 7s 354us/step - loss: 0.7175 - acc: 0.7533\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 2s 119us/step - loss: 0.5201 - acc: 0.7804\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 2s 115us/step - loss: 0.5083 - acc: 0.7807\n",
      "10000/10000 [==============================] - 2s 246us/step\n",
      "20000/20000 [==============================] - 1s 57us/step\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 7s 334us/step - loss: 0.6467 - acc: 0.7668\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 2s 92us/step - loss: 0.5207 - acc: 0.7828\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 2s 90us/step - loss: 0.5145 - acc: 0.7823\n",
      "10000/10000 [==============================] - 2s 246us/step\n",
      "20000/20000 [==============================] - 1s 50us/step\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 7s 331us/step - loss: 0.5998 - acc: 0.7604\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 2s 96us/step - loss: 0.5479 - acc: 0.7713\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 2s 95us/step - loss: 0.5400 - acc: 0.7716\n",
      "10000/10000 [==============================] - 2s 247us/step\n",
      "20000/20000 [==============================] - 1s 55us/step\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 7s 361us/step - loss: 0.5888 - acc: 0.7683\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 2s 101us/step - loss: 0.5127 - acc: 0.7789\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 0.5069 - acc: 0.7808\n",
      "10000/10000 [==============================] - 3s 254us/step\n",
      "20000/20000 [==============================] - 1s 58us/step\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 7s 359us/step - loss: 0.5779 - acc: 0.7760\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 0.5128 - acc: 0.7830\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 2s 98us/step - loss: 0.5067 - acc: 0.7831\n",
      "10000/10000 [==============================] - 3s 255us/step\n",
      "20000/20000 [==============================] - 1s 57us/step\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 7s 365us/step - loss: 0.7537 - acc: 0.7597\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 0.5310 - acc: 0.7719\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 0.5233 - acc: 0.7721\n",
      "10000/10000 [==============================] - 3s 259us/step\n",
      "20000/20000 [==============================] - 1s 57us/step\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 7s 356us/step - loss: 12.4534 - acc: 0.2188\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 2s 96us/step - loss: 12.4534 - acc: 0.2188\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 2s 96us/step - loss: 12.4534 - acc: 0.2188\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 2s 98us/step - loss: 12.4534 - acc: 0.2188\n",
      "10000/10000 [==============================] - 3s 258us/step\n",
      "20000/20000 [==============================] - 1s 58us/step\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 7s 360us/step - loss: 3.4944 - acc: 0.7832\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 3.4944 - acc: 0.7832\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 3.4944 - acc: 0.7832\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 2s 98us/step - loss: 3.4944 - acc: 0.7832\n",
      "10000/10000 [==============================] - 3s 262us/step\n",
      "20000/20000 [==============================] - 1s 61us/step\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 7s 369us/step - loss: 12.3075 - acc: 0.2280\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 12.3075 - acc: 0.2280\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 2s 98us/step - loss: 12.3075 - acc: 0.2280\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 12.3075 - acc: 0.2280\n",
      "10000/10000 [==============================] - 3s 276us/step\n",
      "20000/20000 [==============================] - 1s 59us/step\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 8s 386us/step - loss: 3.6374 - acc: 0.7742\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 2s 103us/step - loss: 3.6324 - acc: 0.7745\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 3.6324 - acc: 0.7745\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 2s 101us/step - loss: 3.6324 - acc: 0.7745\n",
      "10000/10000 [==============================] - 3s 278us/step\n",
      "20000/20000 [==============================] - 1s 60us/step\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 8s 387us/step - loss: 12.4845 - acc: 0.2169\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 2s 101us/step - loss: 12.4845 - acc: 0.2169\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 2s 102us/step - loss: 12.4845 - acc: 0.2169\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 2s 102us/step - loss: 12.4845 - acc: 0.2169\n",
      "10000/10000 [==============================] - 3s 277us/step\n",
      "20000/20000 [==============================] - 1s 60us/step\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 8s 390us/step - loss: 3.6744 - acc: 0.7720\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 2s 102us/step - loss: 3.6734 - acc: 0.7721\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 2s 103us/step - loss: 3.6734 - acc: 0.7721\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 2s 101us/step - loss: 3.6734 - acc: 0.7721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 289us/step\n",
      "20000/20000 [==============================] - 1s 63us/step\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 8s 385us/step - loss: 0.9888 - acc: 0.7414\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 2s 102us/step - loss: 0.5296 - acc: 0.7802\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 2s 101us/step - loss: 0.5196 - acc: 0.7809\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 0.5133 - acc: 0.7810\n",
      "10000/10000 [==============================] - 3s 286us/step\n",
      "20000/20000 [==============================] - 1s 62us/step\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 8s 388us/step - loss: 1.1954 - acc: 0.7374\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 2s 103us/step - loss: 0.5541 - acc: 0.7778\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 3s 125us/step - loss: 0.5212 - acc: 0.7826\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.5134 - acc: 0.7830\n",
      "10000/10000 [==============================] - 3s 328us/step\n",
      "20000/20000 [==============================] - 1s 67us/step\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 8s 401us/step - loss: 0.6419 - acc: 0.7627\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 2s 102us/step - loss: 0.5654 - acc: 0.7690\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 2s 102us/step - loss: 0.5485 - acc: 0.7695\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 2s 104us/step - loss: 0.5427 - acc: 0.7713\n",
      "10000/10000 [==============================] - 3s 296us/step\n",
      "20000/20000 [==============================] - 1s 63us/step\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 8s 415us/step - loss: 0.7992 - acc: 0.7680\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 2s 110us/step - loss: 0.5264 - acc: 0.7801\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 2s 123us/step - loss: 0.5127 - acc: 0.7809\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 2s 115us/step - loss: 0.5076 - acc: 0.7808\n",
      "10000/10000 [==============================] - 3s 301us/step\n",
      "20000/20000 [==============================] - 1s 67us/step\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 8s 413us/step - loss: 2.9413 - acc: 0.7644\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 2s 110us/step - loss: 1.1043 - acc: 0.7209\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 2s 109us/step - loss: 0.6281 - acc: 0.7368\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 2s 108us/step - loss: 0.5829 - acc: 0.7657\n",
      "10000/10000 [==============================] - 3s 303us/step\n",
      "20000/20000 [==============================] - 1s 64us/step\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 8s 417us/step - loss: 1.4379 - acc: 0.7381\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 2s 109us/step - loss: 0.5514 - acc: 0.7662\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 2s 110us/step - loss: 0.5396 - acc: 0.7682\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 2s 111us/step - loss: 0.5314 - acc: 0.7707\n",
      "10000/10000 [==============================] - 3s 310us/step\n",
      "20000/20000 [==============================] - 1s 64us/step\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 8s 412us/step - loss: 0.6610 - acc: 0.7628\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 2s 110us/step - loss: 0.5151 - acc: 0.7808\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 2s 108us/step - loss: 0.5096 - acc: 0.7809\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 2s 106us/step - loss: 0.5046 - acc: 0.7806\n",
      "10000/10000 [==============================] - 3s 308us/step\n",
      "20000/20000 [==============================] - 1s 66us/step\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 8s 410us/step - loss: 0.6867 - acc: 0.7587\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 2s 106us/step - loss: 0.5202 - acc: 0.7831\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 2s 105us/step - loss: 0.5153 - acc: 0.7825\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 2s 105us/step - loss: 0.5112 - acc: 0.7826\n",
      "10000/10000 [==============================] - 3s 312us/step\n",
      "20000/20000 [==============================] - 1s 66us/step\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 8s 418us/step - loss: 0.6988 - acc: 0.7438\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 2s 108us/step - loss: 0.5391 - acc: 0.7711\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 2s 107us/step - loss: 0.5255 - acc: 0.7721\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 2s 106us/step - loss: 0.5255 - acc: 0.7711\n",
      "10000/10000 [==============================] - 3s 318us/step\n",
      "20000/20000 [==============================] - 1s 68us/step\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 9s 431us/step - loss: 0.6554 - acc: 0.7668\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 2s 112us/step - loss: 0.5167 - acc: 0.7801\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 2s 113us/step - loss: 0.5059 - acc: 0.7809\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 2s 114us/step - loss: 0.5030 - acc: 0.7806\n",
      "10000/10000 [==============================] - 3s 322us/step\n",
      "20000/20000 [==============================] - 1s 67us/step\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 9s 433us/step - loss: 0.6107 - acc: 0.7652\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 2s 107us/step - loss: 0.5269 - acc: 0.7814\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 3s 127us/step - loss: 0.5134 - acc: 0.7826\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.5061 - acc: 0.7830\n",
      "10000/10000 [==============================] - 3s 330us/step\n",
      "20000/20000 [==============================] - 1s 68us/step\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 9s 448us/step - loss: 0.5540 - acc: 0.7654\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 2s 115us/step - loss: 0.5273 - acc: 0.7720\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 2s 117us/step - loss: 0.5230 - acc: 0.7720\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 2s 116us/step - loss: 0.5184 - acc: 0.7721\n",
      "10000/10000 [==============================] - 3s 330us/step\n",
      "20000/20000 [==============================] - 1s 74us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 17s 873us/step - loss: 3.5275 - acc: 0.7811\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 11s 548us/step - loss: 3.5274 - acc: 0.7811\n",
      "10000/10000 [==============================] - 6s 612us/step\n",
      "20000/20000 [==============================] - 7s 352us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 19s 941us/step - loss: 5.5543 - acc: 0.6532\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 11s 556us/step - loss: 3.5326 - acc: 0.7807\n",
      "10000/10000 [==============================] - 6s 613us/step\n",
      "20000/20000 [==============================] - 7s 345us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 18s 900us/step - loss: 3.6749 - acc: 0.7720\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 11s 558us/step - loss: 3.6749 - acc: 0.7720\n",
      "10000/10000 [==============================] - 6s 635us/step\n",
      "20000/20000 [==============================] - 7s 357us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 19s 947us/step - loss: 12.4339 - acc: 0.2201\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 12s 577us/step - loss: 12.4479 - acc: 0.2192\n",
      "10000/10000 [==============================] - 7s 656us/step\n",
      "20000/20000 [==============================] - 7s 370us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 19s 967us/step - loss: 3.5482 - acc: 0.7798\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 12s 582us/step - loss: 3.5023 - acc: 0.7827\n",
      "10000/10000 [==============================] - 7s 670us/step\n",
      "20000/20000 [==============================] - 8s 376us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 18s 920us/step - loss: 9.6899 - acc: 0.3930\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 12s 615us/step - loss: 3.7701 - acc: 0.7659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 6s 638us/step\n",
      "20000/20000 [==============================] - 7s 358us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 19s 961us/step - loss: 0.6038 - acc: 0.7703\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 12s 614us/step - loss: 0.5392 - acc: 0.7784\n",
      "10000/10000 [==============================] - 7s 676us/step\n",
      "20000/20000 [==============================] - 8s 421us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 19s 949us/step - loss: 0.8598 - acc: 0.7654\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 13s 645us/step - loss: 0.5420 - acc: 0.7807\n",
      "10000/10000 [==============================] - 7s 745us/step\n",
      "20000/20000 [==============================] - 8s 406us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 19s 971us/step - loss: 0.7255 - acc: 0.7576\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 12s 619us/step - loss: 0.5690 - acc: 0.7695\n",
      "10000/10000 [==============================] - 7s 700us/step\n",
      "20000/20000 [==============================] - 8s 403us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 20s 1ms/step - loss: 0.5933 - acc: 0.7771\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 14s 710us/step - loss: 0.5140 - acc: 0.7810\n",
      "10000/10000 [==============================] - 7s 712us/step\n",
      "20000/20000 [==============================] - 8s 417us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 21s 1ms/step - loss: 0.7187 - acc: 0.7697\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 13s 659us/step - loss: 0.5165 - acc: 0.7830\n",
      "10000/10000 [==============================] - 7s 717us/step\n",
      "20000/20000 [==============================] - 9s 434us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.6664 - acc: 0.7628\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 14s 679us/step - loss: 0.5273 - acc: 0.7719\n",
      "10000/10000 [==============================] - 7s 728us/step\n",
      "20000/20000 [==============================] - 9s 450us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 20s 995us/step - loss: 0.5800 - acc: 0.7792\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 13s 644us/step - loss: 0.5261 - acc: 0.7810\n",
      "10000/10000 [==============================] - 7s 719us/step\n",
      "20000/20000 [==============================] - 9s 434us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 20s 1ms/step - loss: 0.5667 - acc: 0.7791\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 13s 656us/step - loss: 0.5343 - acc: 0.7817\n",
      "10000/10000 [==============================] - 7s 719us/step\n",
      "20000/20000 [==============================] - 9s 452us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 20s 1ms/step - loss: 0.6066 - acc: 0.7643\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 13s 654us/step - loss: 0.5520 - acc: 0.7703\n",
      "10000/10000 [==============================] - 7s 728us/step\n",
      "20000/20000 [==============================] - 9s 436us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 21s 1ms/step - loss: 0.5417 - acc: 0.7765\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 14s 675us/step - loss: 0.5094 - acc: 0.7810\n",
      "10000/10000 [==============================] - 8s 760us/step\n",
      "20000/20000 [==============================] - 9s 440us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 21s 1ms/step - loss: 0.5604 - acc: 0.7769\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 14s 710us/step - loss: 0.5092 - acc: 0.7828\n",
      "10000/10000 [==============================] - 7s 745us/step\n",
      "20000/20000 [==============================] - 9s 438us/step\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 21s 1ms/step - loss: 0.5811 - acc: 0.7677\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 14s 697us/step - loss: 0.5282 - acc: 0.7715\n",
      "10000/10000 [==============================] - 8s 781us/step\n",
      "20000/20000 [==============================] - 9s 464us/step\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 21s 1ms/step - loss: 7.4787 - acc: 0.5324\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 14s 675us/step - loss: 5.3627 - acc: 0.6654\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 15s 742us/step - loss: 3.6044 - acc: 0.7763\n",
      "  450/10000 [>.............................] - ETA: 1:11"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7d178ff10f52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# summarize results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mfit_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;31m# _score will return dict if is_multimetric is True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m         \u001b[0mtest_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_multimetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfit_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer, is_multimetric)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \"\"\"\n\u001b[1;32m    604\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_multimetric\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_multimetric_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_multimetric_score\u001b[0;34m(estimator, X_test, y_test, scorers)\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'item'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/metrics/scorer.py\u001b[0m in \u001b[0;36m_passthrough_scorer\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_passthrough_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;34m\"\"\"Function that wraps estimator.score\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_training/lib/python3.7/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_training/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1111\u001b[0m                                          \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                                          \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1113\u001b[0;31m                                          steps=steps)\n\u001b[0m\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m     def predict(self, x,\n",
      "\u001b[0;32m~/anaconda3/envs/tf_training/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_training/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_training/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`inputs` should be a list or tuple.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0mfeed_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m         \u001b[0marray_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_training/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mcandidate_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_initialized'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m                     \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(optimizer= 'rmsprop' , init= 'glorot_uniform' ):\n",
    "    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=23, kernel_initializer=init, activation= 'relu' ))\n",
    "    model.add(Dense(8, kernel_initializer=init, activation= 'relu' ))\n",
    "    model.add(Dense(1, kernel_initializer=init, activation= 'sigmoid' ))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss= 'binary_crossentropy' , optimizer=optimizer, metrics=[ 'accuracy' ])\n",
    "    return model\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = df.iloc[:,1:24]\n",
    "Y = df.iloc[:,24]\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "# grid search epochs, batch size and optimizer\n",
    "optimizers = [ 'rmsprop' , 'adam' ]\n",
    "inits = [ 'glorot_uniform' , 'normal' , 'uniform' ]\n",
    "epochs = [2, 3, 4]\n",
    "batches = [50, 10, 20]\n",
    "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=inits)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid_result = grid.fit(X, Y)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_[ 'mean_test_score' ]\n",
    "stds = grid_result.cv_results_[ 'std_test_score' ]\n",
    "params = grid_result.cv_results_[ 'params' ]\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Classification Of Flower Species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "\n",
    "# dataset = dataframe.values\n",
    "X = data.data.astype(float)\n",
    "Y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "75/75 [==============================] - 9s 125ms/step - loss: 1.0961 - acc: 0.3333\n",
      "Epoch 2/30\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 1.0947 - acc: 0.3333\n",
      "Epoch 3/30\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.0933 - acc: 0.3333\n",
      "Epoch 4/30\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 1.0921 - acc: 0.3333\n",
      "Epoch 5/30\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 1.0898 - acc: 0.3333\n",
      "Epoch 6/30\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.0878 - acc: 0.3333\n",
      "Epoch 7/30\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.0854 - acc: 0.3333\n",
      "Epoch 8/30\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.0827 - acc: 0.3333\n",
      "Epoch 9/30\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.0797 - acc: 0.3333\n",
      "Epoch 10/30\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.0762 - acc: 0.3333\n",
      "Epoch 11/30\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.0728 - acc: 0.3333\n",
      "Epoch 12/30\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.0686 - acc: 0.3333\n",
      "Epoch 13/30\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.0641 - acc: 0.3333\n",
      "Epoch 14/30\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.0596 - acc: 0.3333\n",
      "Epoch 15/30\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 1.0551 - acc: 0.3333\n",
      "Epoch 16/30\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.0500 - acc: 0.3333\n",
      "Epoch 17/30\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.0442 - acc: 0.3333\n",
      "Epoch 18/30\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 1.0388 - acc: 0.3333\n",
      "Epoch 19/30\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 1.0331 - acc: 0.3333\n",
      "Epoch 20/30\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 1.0267 - acc: 0.3333\n",
      "Epoch 21/30\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.0200 - acc: 0.3333\n",
      "Epoch 22/30\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 1.0137 - acc: 0.3333\n",
      "Epoch 23/30\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 1.0063 - acc: 0.3333\n",
      "Epoch 24/30\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.9993 - acc: 0.3333\n",
      "Epoch 25/30\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.9918 - acc: 0.3333\n",
      "Epoch 26/30\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.9840 - acc: 0.3333\n",
      "Epoch 27/30\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.9763 - acc: 0.3333\n",
      "Epoch 28/30\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.9680 - acc: 0.3467\n",
      "Epoch 29/30\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.9598 - acc: 0.4400\n",
      "Epoch 30/30\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.9517 - acc: 0.5600\n",
      "75/75 [==============================] - 4s 53ms/step\n",
      "Epoch 1/30\n",
      "75/75 [==============================] - 9s 122ms/step - loss: 1.0983 - acc: 0.3333\n",
      "Epoch 2/30\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.0959 - acc: 0.3600\n",
      "Epoch 3/30\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 1.0927 - acc: 0.3600\n",
      "Epoch 4/30\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 1.0890 - acc: 0.3333\n",
      "Epoch 5/30\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 1.0834 - acc: 0.3333\n",
      "Epoch 6/30\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 1.0767 - acc: 0.3333\n",
      "Epoch 7/30\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 1.0688 - acc: 0.3333\n",
      "Epoch 8/30\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.0596 - acc: 0.3333\n",
      "Epoch 9/30\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.0493 - acc: 0.3333\n",
      "Epoch 10/30\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 1.0376 - acc: 0.3467\n",
      "Epoch 11/30\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 1.0276 - acc: 0.3333\n",
      "Epoch 12/30\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 1.0118 - acc: 0.3467\n",
      "Epoch 13/30\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.9978 - acc: 0.3333\n",
      "Epoch 14/30\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.9841 - acc: 0.3333\n",
      "Epoch 15/30\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.9684 - acc: 0.3333\n",
      "Epoch 16/30\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.9533 - acc: 0.3333\n",
      "Epoch 17/30\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.9387 - acc: 0.3467\n",
      "Epoch 18/30\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.9230 - acc: 0.3467\n",
      "Epoch 19/30\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.9092 - acc: 0.3733\n",
      "Epoch 20/30\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.8948 - acc: 0.3467\n",
      "Epoch 21/30\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.8810 - acc: 0.3733\n",
      "Epoch 22/30\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.8684 - acc: 0.3733\n",
      "Epoch 23/30\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.8557 - acc: 0.5200\n",
      "Epoch 24/30\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.8431 - acc: 0.7733\n",
      "Epoch 25/30\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.8324 - acc: 0.7867\n",
      "Epoch 26/30\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.8221 - acc: 0.7867\n",
      "Epoch 27/30\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.8113 - acc: 0.8000\n",
      "Epoch 28/30\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.8016 - acc: 0.8400\n",
      "Epoch 29/30\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.7924 - acc: 0.8667\n",
      "Epoch 30/30\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.7843 - acc: 0.8533\n",
      "75/75 [==============================] - 5s 60ms/step\n",
      "Accuracy: 69.33% (12.00%)\n"
     ]
    }
   ],
   "source": [
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(4, input_dim=4, kernel_initializer= 'normal' , activation= 'relu' ))\n",
    "    model.add(Dense(3, kernel_initializer= 'normal' , activation= 'sigmoid' ))\n",
    "    # Compile model\n",
    "    model.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "    return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=30, batch_size=5, verbose=1)\n",
    "kfold = KFold(n_splits=2, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
    "print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/base.py:467: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "15000/15000 [==============================] - 29s 2ms/step - loss: 0.4711 - acc: 0.8077\n",
      "Epoch 2/10\n",
      "15000/15000 [==============================] - 20s 1ms/step - loss: 0.4506 - acc: 0.8157\n",
      "Epoch 3/10\n",
      "15000/15000 [==============================] - 20s 1ms/step - loss: 0.4462 - acc: 0.8172\n",
      "Epoch 4/10\n",
      "15000/15000 [==============================] - 21s 1ms/step - loss: 0.4419 - acc: 0.8201\n",
      "Epoch 5/10\n",
      "15000/15000 [==============================] - 21s 1ms/step - loss: 0.4395 - acc: 0.8173\n",
      "Epoch 6/10\n",
      "15000/15000 [==============================] - 21s 1ms/step - loss: 0.4379 - acc: 0.8201\n",
      "Epoch 7/10\n",
      "15000/15000 [==============================] - 20s 1ms/step - loss: 0.4366 - acc: 0.8192\n",
      "Epoch 8/10\n",
      "15000/15000 [==============================] - 21s 1ms/step - loss: 0.4350 - acc: 0.8205\n",
      "Epoch 9/10\n",
      "15000/15000 [==============================] - 21s 1ms/step - loss: 0.4344 - acc: 0.8204\n",
      "Epoch 10/10\n",
      "15000/15000 [==============================] - 21s 1ms/step - loss: 0.4342 - acc: 0.8211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/pipeline.py:511: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000/15000 [==============================] - 19s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/base.py:467: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "15000/15000 [==============================] - 31s 2ms/step - loss: 0.4704 - acc: 0.8085\n",
      "Epoch 2/10\n",
      "15000/15000 [==============================] - 21s 1ms/step - loss: 0.4500 - acc: 0.8158\n",
      "Epoch 3/10\n",
      "15000/15000 [==============================] - 21s 1ms/step - loss: 0.4457 - acc: 0.8169\n",
      "Epoch 4/10\n",
      "15000/15000 [==============================] - 22s 1ms/step - loss: 0.4428 - acc: 0.8181\n",
      "Epoch 5/10\n",
      "15000/15000 [==============================] - 21s 1ms/step - loss: 0.4395 - acc: 0.8179\n",
      "Epoch 6/10\n",
      "15000/15000 [==============================] - 22s 1ms/step - loss: 0.4377 - acc: 0.8181\n",
      "Epoch 7/10\n",
      "15000/15000 [==============================] - 23s 2ms/step - loss: 0.4353 - acc: 0.8179\n",
      "Epoch 8/10\n",
      "15000/15000 [==============================] - 22s 1ms/step - loss: 0.4347 - acc: 0.8185\n",
      "Epoch 9/10\n",
      "15000/15000 [==============================] - 21s 1ms/step - loss: 0.4339 - acc: 0.8181\n",
      "Epoch 10/10\n",
      "15000/15000 [==============================] - 21s 1ms/step - loss: 0.4323 - acc: 0.8197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/pipeline.py:511: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000/15000 [==============================] - 20s 1ms/step\n",
      "Standardized: 81.78% (0.16%)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "\n",
    "# baseline model\n",
    "def create_baseline():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=23, kernel_initializer= 'normal' , activation= 'relu' ))\n",
    "    model.add(Dense(1, kernel_initializer= 'normal' , activation= 'sigmoid' ))\n",
    "    # Compile model\n",
    "    model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "    return model\n",
    "\n",
    "# evaluate baseline model with standardized dataset\n",
    "estimators = []\n",
    "estimators.append(( 'standardize' , StandardScaler()))\n",
    "estimators.append(( 'mlp' , KerasClassifier(build_fn=create_baseline, epochs=10,batch_size=5, verbose=1)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
    "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Of Boston House Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/keras-datasets/boston_housing.npz\n",
      "57344/57026 [==============================] - 72s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import boston_housing\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((x_train,x_test),axis = 0)\n",
    "Y = np.concatenate((y_train, y_test),axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define base model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=13, kernel_initializer= 'normal' , activation= 'relu' ))\n",
    "    model.add(Dense(1, kernel_initializer= 'normal' ))\n",
    "    # Compile model\n",
    "    model.compile(loss= 'mean_squared_error' , optimizer= 'adam' )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# evaluate model\n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=20, batch_size=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "253/253 [==============================] - 8s 33ms/step - loss: 536.4551\n",
      "Epoch 2/20\n",
      "253/253 [==============================] - 0s 547us/step - loss: 300.5287\n",
      "Epoch 3/20\n",
      "253/253 [==============================] - 0s 601us/step - loss: 165.6560\n",
      "Epoch 4/20\n",
      "253/253 [==============================] - 0s 558us/step - loss: 144.0992\n",
      "Epoch 5/20\n",
      "253/253 [==============================] - 0s 576us/step - loss: 132.3818\n",
      "Epoch 6/20\n",
      "253/253 [==============================] - 0s 601us/step - loss: 121.2333\n",
      "Epoch 7/20\n",
      "253/253 [==============================] - 0s 597us/step - loss: 111.9869\n",
      "Epoch 8/20\n",
      "253/253 [==============================] - 0s 644us/step - loss: 101.2623\n",
      "Epoch 9/20\n",
      "253/253 [==============================] - 0s 585us/step - loss: 92.0290\n",
      "Epoch 10/20\n",
      "253/253 [==============================] - 0s 543us/step - loss: 86.0694\n",
      "Epoch 11/20\n",
      "253/253 [==============================] - 0s 608us/step - loss: 81.4214\n",
      "Epoch 12/20\n",
      "253/253 [==============================] - 0s 546us/step - loss: 78.4061\n",
      "Epoch 13/20\n",
      "253/253 [==============================] - 0s 616us/step - loss: 76.0565\n",
      "Epoch 14/20\n",
      "253/253 [==============================] - 0s 601us/step - loss: 75.8998\n",
      "Epoch 15/20\n",
      "253/253 [==============================] - 0s 603us/step - loss: 74.5766\n",
      "Epoch 16/20\n",
      "253/253 [==============================] - 0s 569us/step - loss: 73.9507\n",
      "Epoch 17/20\n",
      "253/253 [==============================] - 0s 779us/step - loss: 73.2679\n",
      "Epoch 18/20\n",
      "253/253 [==============================] - 0s 766us/step - loss: 72.6924\n",
      "Epoch 19/20\n",
      "253/253 [==============================] - 0s 582us/step - loss: 72.9854\n",
      "Epoch 20/20\n",
      "253/253 [==============================] - 0s 616us/step - loss: 72.1988\n",
      "253/253 [==============================] - 4s 15ms/step\n",
      "Epoch 1/20\n",
      "253/253 [==============================] - 8s 33ms/step - loss: 573.7335\n",
      "Epoch 2/20\n",
      "253/253 [==============================] - 0s 598us/step - loss: 237.2763\n",
      "Epoch 3/20\n",
      "253/253 [==============================] - 0s 613us/step - loss: 96.0226\n",
      "Epoch 4/20\n",
      "253/253 [==============================] - 0s 616us/step - loss: 84.3880\n",
      "Epoch 5/20\n",
      "253/253 [==============================] - 0s 628us/step - loss: 78.0564\n",
      "Epoch 6/20\n",
      "253/253 [==============================] - 0s 633us/step - loss: 72.2939\n",
      "Epoch 7/20\n",
      "253/253 [==============================] - 0s 626us/step - loss: 67.9204\n",
      "Epoch 8/20\n",
      "253/253 [==============================] - 0s 611us/step - loss: 64.5064\n",
      "Epoch 9/20\n",
      "253/253 [==============================] - 0s 641us/step - loss: 62.0631\n",
      "Epoch 10/20\n",
      "253/253 [==============================] - 0s 609us/step - loss: 58.4194\n",
      "Epoch 11/20\n",
      "253/253 [==============================] - 0s 597us/step - loss: 58.0327\n",
      "Epoch 12/20\n",
      "253/253 [==============================] - 0s 703us/step - loss: 55.3078\n",
      "Epoch 13/20\n",
      "253/253 [==============================] - 0s 622us/step - loss: 54.1783\n",
      "Epoch 14/20\n",
      "253/253 [==============================] - 0s 609us/step - loss: 53.4496\n",
      "Epoch 15/20\n",
      "253/253 [==============================] - 0s 669us/step - loss: 52.3545\n",
      "Epoch 16/20\n",
      "253/253 [==============================] - 0s 615us/step - loss: 51.9039\n",
      "Epoch 17/20\n",
      "253/253 [==============================] - 0s 618us/step - loss: 51.8180\n",
      "Epoch 18/20\n",
      "253/253 [==============================] - 0s 643us/step - loss: 52.0486\n",
      "Epoch 19/20\n",
      "253/253 [==============================] - 0s 592us/step - loss: 50.0675\n",
      "Epoch 20/20\n",
      "253/253 [==============================] - 0s 671us/step - loss: 49.7277\n",
      "253/253 [==============================] - 4s 14ms/step\n",
      "Baseline: -63.26 (5.46) MSE\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=2, random_state=seed)\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold)\n",
    "print(\"Baseline: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### standardize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "253/253 [==============================] - 9s 35ms/step - loss: 631.5473\n",
      "Epoch 2/20\n",
      "253/253 [==============================] - 0s 609us/step - loss: 626.7533\n",
      "Epoch 3/20\n",
      "253/253 [==============================] - 0s 825us/step - loss: 618.8677\n",
      "Epoch 4/20\n",
      "253/253 [==============================] - 0s 575us/step - loss: 606.5708\n",
      "Epoch 5/20\n",
      "253/253 [==============================] - 0s 582us/step - loss: 589.6159\n",
      "Epoch 6/20\n",
      "253/253 [==============================] - 0s 770us/step - loss: 567.5758\n",
      "Epoch 7/20\n",
      "253/253 [==============================] - 0s 657us/step - loss: 539.7418\n",
      "Epoch 8/20\n",
      "253/253 [==============================] - 0s 735us/step - loss: 507.6641\n",
      "Epoch 9/20\n",
      "253/253 [==============================] - 0s 846us/step - loss: 472.5216\n",
      "Epoch 10/20\n",
      "253/253 [==============================] - 0s 600us/step - loss: 434.8455\n",
      "Epoch 11/20\n",
      "253/253 [==============================] - 0s 635us/step - loss: 396.7358\n",
      "Epoch 12/20\n",
      "253/253 [==============================] - 0s 612us/step - loss: 357.3667\n",
      "Epoch 13/20\n",
      "253/253 [==============================] - 0s 660us/step - loss: 319.9475\n",
      "Epoch 14/20\n",
      "253/253 [==============================] - 0s 609us/step - loss: 283.8288\n",
      "Epoch 15/20\n",
      "253/253 [==============================] - 0s 630us/step - loss: 250.6775\n",
      "Epoch 16/20\n",
      "253/253 [==============================] - 0s 626us/step - loss: 218.7527\n",
      "Epoch 17/20\n",
      "253/253 [==============================] - 0s 641us/step - loss: 191.2678\n",
      "Epoch 18/20\n",
      "253/253 [==============================] - 0s 625us/step - loss: 166.5904\n",
      "Epoch 19/20\n",
      "253/253 [==============================] - 0s 589us/step - loss: 145.6030\n",
      "Epoch 20/20\n",
      "253/253 [==============================] - 0s 804us/step - loss: 127.9867\n",
      "253/253 [==============================] - 4s 15ms/step\n",
      "Epoch 1/20\n",
      "253/253 [==============================] - 9s 35ms/step - loss: 549.3023\n",
      "Epoch 2/20\n",
      "253/253 [==============================] - 0s 579us/step - loss: 545.1911\n",
      "Epoch 3/20\n",
      "253/253 [==============================] - 0s 594us/step - loss: 538.8612\n",
      "Epoch 4/20\n",
      "253/253 [==============================] - 0s 563us/step - loss: 529.3470\n",
      "Epoch 5/20\n",
      "253/253 [==============================] - 0s 599us/step - loss: 515.4591\n",
      "Epoch 6/20\n",
      "253/253 [==============================] - 0s 628us/step - loss: 497.3400\n",
      "Epoch 7/20\n",
      "253/253 [==============================] - 0s 595us/step - loss: 473.3569\n",
      "Epoch 8/20\n",
      "253/253 [==============================] - 0s 617us/step - loss: 444.2441\n",
      "Epoch 9/20\n",
      "253/253 [==============================] - 0s 626us/step - loss: 411.4677\n",
      "Epoch 10/20\n",
      "253/253 [==============================] - 0s 587us/step - loss: 375.8563\n",
      "Epoch 11/20\n",
      "253/253 [==============================] - 0s 654us/step - loss: 339.4512\n",
      "Epoch 12/20\n",
      "253/253 [==============================] - 0s 620us/step - loss: 304.4337\n",
      "Epoch 13/20\n",
      "253/253 [==============================] - 0s 602us/step - loss: 269.9713\n",
      "Epoch 14/20\n",
      "253/253 [==============================] - 0s 602us/step - loss: 238.0603\n",
      "Epoch 15/20\n",
      "253/253 [==============================] - 0s 603us/step - loss: 208.1045\n",
      "Epoch 16/20\n",
      "253/253 [==============================] - 0s 673us/step - loss: 182.4104\n",
      "Epoch 17/20\n",
      "253/253 [==============================] - 0s 603us/step - loss: 160.3128\n",
      "Epoch 18/20\n",
      "253/253 [==============================] - 0s 603us/step - loss: 140.1476\n",
      "Epoch 19/20\n",
      "253/253 [==============================] - 0s 633us/step - loss: 123.2557\n",
      "Epoch 20/20\n",
      "253/253 [==============================] - 0s 666us/step - loss: 108.8414\n",
      "253/253 [==============================] - 4s 14ms/step\n",
      "Standardized: -117.08 (24.50) MSE\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.datasets import boston_housing\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "X = np.concatenate((x_train,x_test),axis = 0)\n",
    "Y = np.concatenate((y_train, y_test),axis = 0)\n",
    "\n",
    "# define base model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=13, kernel_initializer= 'normal' , activation= 'relu' ))\n",
    "    model.add(Dense(1, kernel_initializer= 'normal' ))\n",
    "    # Compile model\n",
    "    model.compile(loss= 'mean_squared_error' , optimizer= 'adam' )\n",
    "    return model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# evaluate model with standardized dataset\n",
    "estimators = []\n",
    "estimators.append(( 'standardize' , StandardScaler()))\n",
    "estimators.append(( 'mlp' , KerasRegressor(build_fn=baseline_model, epochs=20, batch_size=10,verbose=1)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=2, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
    "print(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Your Neural Network Model to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "30000/30000 [==============================] - 27s 907us/step - loss: 0.5535 - acc: 0.7764\n",
      "Epoch 2/20\n",
      "30000/30000 [==============================] - 19s 625us/step - loss: 0.5157 - acc: 0.7787\n",
      "Epoch 3/20\n",
      "30000/30000 [==============================] - 19s 619us/step - loss: 0.5268 - acc: 0.7788\n",
      "Epoch 4/20\n",
      "30000/30000 [==============================] - 21s 686us/step - loss: 0.5277 - acc: 0.7788\n",
      "Epoch 5/20\n",
      "30000/30000 [==============================] - 20s 683us/step - loss: 0.5285 - acc: 0.7788\n",
      "Epoch 6/20\n",
      "30000/30000 [==============================] - 22s 720us/step - loss: 0.5285 - acc: 0.7788\n",
      "Epoch 7/20\n",
      "30000/30000 [==============================] - 21s 697us/step - loss: 0.5285 - acc: 0.7788\n",
      "Epoch 8/20\n",
      "30000/30000 [==============================] - 22s 743us/step - loss: 0.5285 - acc: 0.7788\n",
      "Epoch 9/20\n",
      "30000/30000 [==============================] - 20s 675us/step - loss: 0.5284 - acc: 0.7788\n",
      "Epoch 10/20\n",
      "30000/30000 [==============================] - 20s 659us/step - loss: 0.5285 - acc: 0.7788\n",
      "Epoch 11/20\n",
      "30000/30000 [==============================] - 20s 654us/step - loss: 0.5285 - acc: 0.7788\n",
      "Epoch 12/20\n",
      "30000/30000 [==============================] - 20s 660us/step - loss: 0.5285 - acc: 0.7788\n",
      "Epoch 13/20\n",
      "30000/30000 [==============================] - 19s 641us/step - loss: 0.5285 - acc: 0.7788\n",
      "Epoch 14/20\n",
      "30000/30000 [==============================] - 19s 647us/step - loss: 0.5285 - acc: 0.7788\n",
      "Epoch 15/20\n",
      "30000/30000 [==============================] - 19s 643us/step - loss: 0.5285 - acc: 0.7788\n",
      "Epoch 16/20\n",
      "30000/30000 [==============================] - 19s 634us/step - loss: 0.5285 - acc: 0.7788\n",
      "Epoch 17/20\n",
      "30000/30000 [==============================] - 19s 645us/step - loss: 0.5285 - acc: 0.7788\n",
      "Epoch 18/20\n",
      "30000/30000 [==============================] - 21s 689us/step - loss: 0.5285 - acc: 0.7788\n",
      "Epoch 19/20\n",
      "30000/30000 [==============================] - 21s 688us/step - loss: 0.5285 - acc: 0.7788\n",
      "Epoch 20/20\n",
      "30000/30000 [==============================] - 19s 637us/step - loss: 0.5285 - acc: 0.7788\n",
      "acc: 77.88%\n",
      "Saved model to disk\n",
      "Loaded model from disk\n",
      "acc: 77.88%\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import model_from_json\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import os\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load  dataset\n",
    "df = pd.read_excel('Credit Card _Dataset.xls')\n",
    "df.head()\n",
    "X = df.iloc[:,1:24]\n",
    "Y = df.iloc[:,24]\n",
    "X.set_index(np.arange(30000), inplace = True)\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=23, kernel_initializer= 'uniform' , activation= 'relu' ))\n",
    "model.add(Dense(8, kernel_initializer= 'uniform' , activation= 'relu' ))\n",
    "model.add(Dense(1, kernel_initializer= 'uniform' , activation= 'sigmoid' ))\n",
    "# Compile model\n",
    "model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "# Fit the model\n",
    "model.fit(X, Y, epochs=20, batch_size=10, verbose=1)\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "# later...\n",
    "# load json and create model\n",
    "json_file = open( 'model.json' , 'r' )\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss= 'binary_crossentropy' , optimizer= 'rmsprop' , metrics=[ 'accuracy' ])\n",
    "score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint Model Improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20099 samples, validate on 9901 samples\n",
      "Epoch 1/15\n",
      " - 24s - loss: 0.5702 - acc: 0.7675 - val_loss: 0.4942 - val_acc: 0.7914\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/keras/callbacks.py:434: RuntimeWarning: Can save best model only with  val_acc  available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 13s - loss: 0.5238 - acc: 0.7726 - val_loss: 0.4974 - val_acc: 0.7914\n",
      "Epoch 3/15\n",
      " - 13s - loss: 0.5318 - acc: 0.7726 - val_loss: 0.5177 - val_acc: 0.7914\n",
      "Epoch 4/15\n",
      " - 14s - loss: 0.5366 - acc: 0.7726 - val_loss: 0.5137 - val_acc: 0.7914\n",
      "Epoch 5/15\n",
      " - 14s - loss: 0.5362 - acc: 0.7726 - val_loss: 0.5128 - val_acc: 0.7914\n",
      "Epoch 6/15\n",
      " - 14s - loss: 0.5362 - acc: 0.7726 - val_loss: 0.5130 - val_acc: 0.7914\n",
      "Epoch 7/15\n",
      " - 15s - loss: 0.5362 - acc: 0.7726 - val_loss: 0.5130 - val_acc: 0.7914\n",
      "Epoch 8/15\n",
      " - 15s - loss: 0.5362 - acc: 0.7726 - val_loss: 0.5132 - val_acc: 0.7914\n",
      "Epoch 9/15\n",
      " - 14s - loss: 0.5362 - acc: 0.7726 - val_loss: 0.5130 - val_acc: 0.7914\n",
      "Epoch 10/15\n",
      " - 13s - loss: 0.5359 - acc: 0.7725 - val_loss: 0.5113 - val_acc: 0.7914\n",
      "Epoch 11/15\n",
      " - 15s - loss: 0.5544 - acc: 0.7724 - val_loss: 0.5165 - val_acc: 0.7914\n",
      "Epoch 12/15\n",
      " - 15s - loss: 0.5386 - acc: 0.7726 - val_loss: 0.5159 - val_acc: 0.7914\n",
      "Epoch 13/15\n",
      " - 16s - loss: 0.5384 - acc: 0.7726 - val_loss: 0.5156 - val_acc: 0.7914\n",
      "Epoch 14/15\n",
      " - 14s - loss: 0.5383 - acc: 0.7726 - val_loss: 0.5167 - val_acc: 0.7914\n",
      "Epoch 15/15\n",
      " - 15s - loss: 0.5384 - acc: 0.7726 - val_loss: 0.5163 - val_acc: 0.7914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9524130c18>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checkpoint the weights when validation accuracy improves\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import pandas as pd\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load pima indians dataset\n",
    "df = pd.read_excel('Credit Card _Dataset.xls')\n",
    "df.head()\n",
    "X = df.iloc[:,1:24]\n",
    "Y = df.iloc[:,24]\n",
    "X.set_index(np.arange(30000), inplace = True)\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=23, kernel_initializer= 'uniform' , activation= 'relu' ))\n",
    "model.add(Dense(8, kernel_initializer= 'uniform' , activation= 'relu' ))\n",
    "model.add(Dense(1, kernel_initializer= 'uniform' , activation= 'sigmoid' ))\n",
    "# Compile model\n",
    "model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "# checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor= ' val_acc ' , verbose=1, save_best_only=True,mode= 'max' )\n",
    "callbacks_list = [checkpoint]\n",
    "# Fit the model\n",
    "model.fit(X, Y, validation_split=0.33, epochs=15, batch_size=10, callbacks=callbacks_list,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20099 samples, validate on 9901 samples\n",
      "Epoch 1/15\n",
      "20099/20099 [==============================] - 28s 1ms/step - loss: 0.5644 - acc: 0.7681 - val_loss: 0.4996 - val_acc: 0.7914\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.79144, saving model to weights.best.hdf5\n",
      "Epoch 2/15\n",
      "20099/20099 [==============================] - 18s 900us/step - loss: 0.5266 - acc: 0.7725 - val_loss: 0.5150 - val_acc: 0.7914\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.79144\n",
      "Epoch 3/15\n",
      "20099/20099 [==============================] - 19s 921us/step - loss: 0.5240 - acc: 0.7725 - val_loss: 0.4942 - val_acc: 0.7914\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.79144\n",
      "Epoch 4/15\n",
      "20099/20099 [==============================] - 17s 837us/step - loss: 0.5211 - acc: 0.7726 - val_loss: 0.4980 - val_acc: 0.7914\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.79144\n",
      "Epoch 5/15\n",
      "20099/20099 [==============================] - 18s 904us/step - loss: 0.5240 - acc: 0.7726 - val_loss: 0.4926 - val_acc: 0.7914\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.79144\n",
      "Epoch 6/15\n",
      "20099/20099 [==============================] - 17s 860us/step - loss: 0.5254 - acc: 0.7726 - val_loss: 0.5223 - val_acc: 0.7914\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.79144\n",
      "Epoch 7/15\n",
      "20099/20099 [==============================] - 18s 904us/step - loss: 0.5371 - acc: 0.7726 - val_loss: 0.5132 - val_acc: 0.7914\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.79144\n",
      "Epoch 8/15\n",
      "20099/20099 [==============================] - 17s 835us/step - loss: 0.5358 - acc: 0.7726 - val_loss: 0.5127 - val_acc: 0.7914\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.79144\n",
      "Epoch 9/15\n",
      "20099/20099 [==============================] - 17s 846us/step - loss: 0.5350 - acc: 0.7726 - val_loss: 0.5101 - val_acc: 0.7914\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.79144\n",
      "Epoch 10/15\n",
      "20099/20099 [==============================] - 17s 827us/step - loss: 0.5334 - acc: 0.7722 - val_loss: 0.5065 - val_acc: 0.7914\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.79144\n",
      "Epoch 11/15\n",
      "20099/20099 [==============================] - 15s 760us/step - loss: 0.5291 - acc: 0.7726 - val_loss: 0.5063 - val_acc: 0.7914\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.79144\n",
      "Epoch 12/15\n",
      "20099/20099 [==============================] - 15s 745us/step - loss: 0.5294 - acc: 0.7726 - val_loss: 0.5068 - val_acc: 0.7914\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.79144\n",
      "Epoch 13/15\n",
      "20099/20099 [==============================] - 15s 758us/step - loss: 0.5315 - acc: 0.7726 - val_loss: 0.5055 - val_acc: 0.7914\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.79144\n",
      "Epoch 14/15\n",
      "20099/20099 [==============================] - 16s 771us/step - loss: 0.5293 - acc: 0.7726 - val_loss: 0.5036 - val_acc: 0.7914\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.79144\n",
      "Epoch 15/15\n",
      "20099/20099 [==============================] - 16s 774us/step - loss: 0.5287 - acc: 0.7726 - val_loss: 0.5052 - val_acc: 0.7914\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.79144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9522d7acf8>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checkpoint the weights when validation accuracy improves\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import pandas as pd\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load pima indians dataset\n",
    "df = pd.read_excel('Credit Card _Dataset.xls')\n",
    "df.head()\n",
    "X = df.iloc[:,1:24]\n",
    "Y = df.iloc[:,24]\n",
    "X.set_index(np.arange(30000), inplace = True)\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=23, kernel_initializer= 'uniform' , activation= 'relu' ))\n",
    "model.add(Dense(8, kernel_initializer= 'uniform' , activation= 'relu' ))\n",
    "model.add(Dense(1, kernel_initializer= 'uniform' , activation= 'sigmoid' ))\n",
    "# Compile model\n",
    "model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "# checkpoint\n",
    "filepath=\"weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor= 'val_acc' , verbose=1, save_best_only=True,mode= 'max' )\n",
    "callbacks_list = [checkpoint]\n",
    "# Fit the model\n",
    "model.fit(X, Y, validation_split=0.33, epochs=15, batch_size=10, callbacks=callbacks_list,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Model Training History in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20099 samples, validate on 9901 samples\n",
      "Epoch 1/5\n",
      "20099/20099 [==============================] - 27s 1ms/step - loss: 0.5917 - acc: 0.7670 - val_loss: 0.5055 - val_acc: 0.7914\n",
      "Epoch 2/5\n",
      "20099/20099 [==============================] - 17s 856us/step - loss: 0.5354 - acc: 0.7720 - val_loss: 0.5170 - val_acc: 0.7913\n",
      "Epoch 3/5\n",
      "20099/20099 [==============================] - 18s 877us/step - loss: 0.5364 - acc: 0.7726 - val_loss: 0.5131 - val_acc: 0.7914\n",
      "Epoch 4/5\n",
      "20099/20099 [==============================] - 18s 906us/step - loss: 0.5362 - acc: 0.7726 - val_loss: 0.5134 - val_acc: 0.7914\n",
      "Epoch 5/5\n",
      "20099/20099 [==============================] - 17s 848us/step - loss: 0.5388 - acc: 0.7724 - val_loss: 0.5128 - val_acc: 0.7914\n",
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize training history\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load pima indians dataset\n",
    "df = pd.read_excel('Credit Card _Dataset.xls')\n",
    "df.head()\n",
    "X = df.iloc[:,1:24]\n",
    "Y = df.iloc[:,24]\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=23, kernel_initializer= 'uniform' , activation= 'relu' ))\n",
    "model.add(Dense(8, kernel_initializer= 'uniform' , activation= 'relu' ))\n",
    "model.add(Dense(1, kernel_initializer= 'uniform' , activation= 'sigmoid' ))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X, Y, validation_split=0.33, epochs=5, batch_size=10, verbose=1)\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history[ 'acc' ])\n",
    "plt.plot(history.history[ 'val_acc' ])\n",
    "plt.title( 'model accuracy' )\n",
    "plt.ylabel( 'accuracy' )\n",
    "plt.xlabel( 'epoch' )\n",
    "plt.legend([ 'train' , 'test' ], loc= 'upper left' )\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history[ 'loss' ])\n",
    "plt.plot(history.history[ 'val_loss' ])\n",
    "plt.title( 'model loss' )\n",
    "plt.ylabel( 'loss')\n",
    "plt.xlabel( 'epoch' )\n",
    "plt.legend([ 'train' , 'test' ], loc= 'upper left' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout Regularization in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/base.py:467: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 18s 896us/step - loss: 0.5054 - acc: 0.7857\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 8s 421us/step - loss: 0.4487 - acc: 0.8149\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 9s 452us/step - loss: 0.4420 - acc: 0.8178\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 9s 426us/step - loss: 0.4378 - acc: 0.8181\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 9s 451us/step - loss: 0.4359 - acc: 0.8193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/pipeline.py:511: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 8s 763us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/base.py:467: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 20s 989us/step - loss: 0.4847 - acc: 0.7954\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 9s 437us/step - loss: 0.4427 - acc: 0.8178\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 9s 432us/step - loss: 0.4371 - acc: 0.8212\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 9s 431us/step - loss: 0.4345 - acc: 0.8199\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 9s 431us/step - loss: 0.4320 - acc: 0.8224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/pipeline.py:511: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 722us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/base.py:467: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 19s 940us/step - loss: 0.4852 - acc: 0.7970\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 9s 463us/step - loss: 0.4436 - acc: 0.8178\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 9s 470us/step - loss: 0.4381 - acc: 0.8195\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 9s 433us/step - loss: 0.4354 - acc: 0.8209\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 9s 445us/step - loss: 0.4334 - acc: 0.8184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/pipeline.py:511: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 743us/step\n",
      "Baseline: 81.91% (0.24%)\n"
     ]
    }
   ],
   "source": [
    "# Baseline Model on the Sonar Dataset\n",
    "import numpy\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load dataset\n",
    "df = pd.read_excel('Credit Card _Dataset.xls')\n",
    "df.head()\n",
    "X = df.iloc[:,1:24]\n",
    "Y = df.iloc[:,24]\n",
    "\n",
    "\n",
    "# baseline\n",
    "def create_baseline():\n",
    "# create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=23, kernel_initializer= 'normal' , activation= 'relu' ))\n",
    "    model.add(Dense(30, kernel_initializer= 'normal' , activation= 'relu' ))\n",
    "    model.add(Dense(1, kernel_initializer= 'normal' , activation= 'sigmoid' ))\n",
    "    # Compile model\n",
    "    sgd = SGD(lr=0.01, momentum=0.8, decay=0.0, nesterov=False)\n",
    "    model.compile(loss= 'binary_crossentropy' , optimizer=sgd, metrics=[ 'accuracy' ])\n",
    "    return model\n",
    "\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(( 'standardize' , StandardScaler()))\n",
    "estimators.append(( 'mlp' , KerasClassifier(build_fn=create_baseline, epochs=5,batch_size=16, verbose=1)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X,Y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/base.py:467: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "26999/26999 [==============================] - 22s 797us/step - loss: 0.4837 - acc: 0.7917\n",
      "Epoch 2/3\n",
      "26999/26999 [==============================] - 12s 437us/step - loss: 0.4746 - acc: 0.7967\n",
      "Epoch 3/3\n",
      "26999/26999 [==============================] - 12s 433us/step - loss: 0.4700 - acc: 0.7963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/pipeline.py:511: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3001/3001 [==============================] - 5s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/base.py:467: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "26999/26999 [==============================] - 21s 795us/step - loss: 0.4840 - acc: 0.7890\n",
      "Epoch 2/3\n",
      "26999/26999 [==============================] - 12s 445us/step - loss: 0.4717 - acc: 0.7954\n",
      "Epoch 3/3\n",
      "26999/26999 [==============================] - 13s 480us/step - loss: 0.4706 - acc: 0.7967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/pipeline.py:511: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3001/3001 [==============================] - 6s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/base.py:467: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "26999/26999 [==============================] - 22s 821us/step - loss: 0.4860 - acc: 0.7904\n",
      "Epoch 2/3\n",
      "26999/26999 [==============================] - 12s 441us/step - loss: 0.4748 - acc: 0.7941\n",
      "Epoch 3/3\n",
      "26999/26999 [==============================] - 12s 452us/step - loss: 0.4723 - acc: 0.7923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/pipeline.py:511: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3001/3001 [==============================] - 6s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/base.py:467: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "26999/26999 [==============================] - 22s 830us/step - loss: 0.4859 - acc: 0.7950\n",
      "Epoch 2/3\n",
      "26999/26999 [==============================] - 12s 446us/step - loss: 0.4741 - acc: 0.7938\n",
      "Epoch 3/3\n",
      "26999/26999 [==============================] - 12s 451us/step - loss: 0.4739 - acc: 0.7924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/pipeline.py:511: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3001/3001 [==============================] - 5s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/base.py:467: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "27000/27000 [==============================] - 22s 821us/step - loss: 0.4840 - acc: 0.7907\n",
      "Epoch 2/3\n",
      "27000/27000 [==============================] - 12s 448us/step - loss: 0.4771 - acc: 0.7964\n",
      "Epoch 3/3\n",
      "27000/27000 [==============================] - 12s 443us/step - loss: 0.4705 - acc: 0.7973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/pipeline.py:511: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 5s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/base.py:467: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "27000/27000 [==============================] - 23s 864us/step - loss: 0.4814 - acc: 0.7958\n",
      "Epoch 2/3\n",
      "27000/27000 [==============================] - 13s 485us/step - loss: 0.4787 - acc: 0.7895\n",
      "Epoch 3/3\n",
      "27000/27000 [==============================] - 13s 472us/step - loss: 0.4724 - acc: 0.7942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/pipeline.py:511: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 5s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/base.py:467: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "27001/27001 [==============================] - 22s 805us/step - loss: 0.4798 - acc: 0.7969\n",
      "Epoch 2/3\n",
      "27001/27001 [==============================] - 10s 388us/step - loss: 0.4755 - acc: 0.7990\n",
      "Epoch 3/3\n",
      "27001/27001 [==============================] - 11s 395us/step - loss: 0.4729 - acc: 0.7947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/pipeline.py:511: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999/2999 [==============================] - 6s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/base.py:467: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "27001/27001 [==============================] - 25s 943us/step - loss: 0.4828 - acc: 0.7936\n",
      "Epoch 2/3\n",
      "27001/27001 [==============================] - 12s 454us/step - loss: 0.4730 - acc: 0.7949\n",
      "Epoch 3/3\n",
      "27001/27001 [==============================] - 12s 458us/step - loss: 0.4699 - acc: 0.7925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/pipeline.py:511: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999/2999 [==============================] - 5s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/base.py:467: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "27001/27001 [==============================] - 23s 853us/step - loss: 0.4819 - acc: 0.7935\n",
      "Epoch 2/3\n",
      "27001/27001 [==============================] - 13s 463us/step - loss: 0.4748 - acc: 0.7903\n",
      "Epoch 3/3\n",
      "27001/27001 [==============================] - 13s 481us/step - loss: 0.4718 - acc: 0.7949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/pipeline.py:511: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999/2999 [==============================] - 5s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/base.py:467: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "27001/27001 [==============================] - 23s 843us/step - loss: 0.4851 - acc: 0.7886\n",
      "Epoch 2/3\n",
      "27001/27001 [==============================] - 12s 457us/step - loss: 0.4792 - acc: 0.7903\n",
      "Epoch 3/3\n",
      "27001/27001 [==============================] - 12s 461us/step - loss: 0.4741 - acc: 0.7886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/sklearn/pipeline.py:511: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999/2999 [==============================] - 5s 2ms/step\n",
      "Visible: 79.58% (1.60%)\n"
     ]
    }
   ],
   "source": [
    "# Example of Dropout on the Sonar Dataset: Visible Layer\n",
    "import numpy\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load dataset\n",
    "df = pd.read_excel('Credit Card _Dataset.xls')\n",
    "df.head()\n",
    "X = df.iloc[:,1:24]\n",
    "Y = df.iloc[:,24]\n",
    "\n",
    "\n",
    "# dropout in the input layer with weight constraint\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.2, input_shape=(23,)))\n",
    "    model.add(Dense(60, kernel_initializer= 'normal' , activation= 'relu' ,\n",
    "    kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dense(30, kernel_initializer= 'normal' , activation= 'relu' ,\n",
    "    kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dense(1, kernel_initializer= 'normal' , activation= 'sigmoid' ))\n",
    "    # Compile model\n",
    "    sgd = SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=False)\n",
    "    model.compile(loss= 'binary_crossentropy' , optimizer=sgd, metrics=[ 'accuracy' ])\n",
    "    return model\n",
    "\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(( 'standardize' , StandardScaler()))\n",
    "estimators.append(( 'mlp' , KerasClassifier(build_fn=create_baseline, epochs=5,batch_size=16, verbose=1)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
    "print(\"Visible: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 4s 178us/step - loss: 0.4942 - acc: 0.7966\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 3s 126us/step - loss: 0.4854 - acc: 0.7832\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.4857 - acc: 0.7790\n",
      "10000/10000 [==============================] - 1s 59us/step\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 3s 158us/step - loss: 0.4900 - acc: 0.7959\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 3s 130us/step - loss: 0.4966 - acc: 0.7867\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 3s 127us/step - loss: 0.4952 - acc: 0.7779\n",
      "10000/10000 [==============================] - 1s 58us/step\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 0.4903 - acc: 0.7915\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 3s 131us/step - loss: 0.4842 - acc: 0.7792\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 3s 127us/step - loss: 0.4844 - acc: 0.7822\n",
      "10000/10000 [==============================] - 1s 54us/step\n",
      "Hidden: 77.88% (0.00%)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "\n",
    "# load dataset\n",
    "df = pd.read_excel('Credit Card _Dataset.xls')\n",
    "df.head()\n",
    "X = df.iloc[:,1:24]\n",
    "Y = df.iloc[:,24]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# dropout in hidden layers with weight constraint\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=23,kernel_initializer='normal',activation='relu',kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(30, kernel_initializer= 'normal' , activation= 'relu' ,kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, kernel_initializer= 'normal' , activation= 'sigmoid' ))\n",
    "    # Compile model\n",
    "    sgd = SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=False)\n",
    "    model.compile(loss= 'binary_crossentropy' , optimizer=sgd, metrics=[ 'accuracy' ])\n",
    "    return model\n",
    "\n",
    "\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(( 'standardize' , StandardScaler()))\n",
    "estimators.append(( 'mlp' , KerasClassifier(build_fn=create_model, epochs=3, batch_size=16,verbose=1)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
    "print(\"Hidden: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-Based Learning Rate Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20099 samples, validate on 9901 samples\n",
      "Epoch 1/5\n",
      " - 1s - loss: 3.6701 - acc: 0.7723 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 2/5\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 3/5\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 4/5\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 5/5\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f91487275f8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load dataset\n",
    "df = pd.read_excel('Credit Card _Dataset.xls')\n",
    "df.head()\n",
    "X = df.iloc[:,1:24]\n",
    "Y = df.iloc[:,24]\n",
    "\n",
    "\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(34, input_dim=23, kernel_initializer= 'normal' , activation= 'relu' ))\n",
    "model.add(Dense(1, kernel_initializer= 'normal' , activation= 'sigmoid' ))\n",
    "\n",
    "\n",
    "# Compile model\n",
    "epochs = 5\n",
    "learning_rate = 0.1\n",
    "decay_rate = learning_rate / epochs\n",
    "momentum = 0.8\n",
    "sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "model.compile(loss= 'binary_crossentropy' , optimizer=sgd, metrics=[ 'accuracy' ])\n",
    "# Fit the model\n",
    "model.fit(X, Y, validation_split=0.33, epochs=epochs, batch_size=28, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop-Based Learning Rate Decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20099 samples, validate on 9901 samples\n",
      "Epoch 1/50\n",
      " - 2s - loss: 3.6701 - acc: 0.7723 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 2/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 3/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 4/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 5/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 6/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 7/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 8/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 9/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 10/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 11/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 12/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 13/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 14/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 15/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 16/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 17/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 18/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 19/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 20/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 21/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 22/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 23/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 24/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 25/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 26/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 27/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 28/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 29/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 30/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 31/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 32/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 33/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 34/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 35/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 36/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 37/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 38/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 39/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 40/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 41/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 42/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 43/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 44/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 45/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 46/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 47/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 48/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 49/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n",
      "Epoch 50/50\n",
      " - 1s - loss: 3.6656 - acc: 0.7726 - val_loss: 3.3617 - val_acc: 0.7914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9148fedcc0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop-Based Learning Rate Decay\n",
    "import pandas\n",
    "import numpy\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "# learning rate schedule\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load dataset\n",
    "df = pd.read_excel('Credit Card _Dataset.xls')\n",
    "df.head()\n",
    "X = df.iloc[:,1:24]\n",
    "Y = df.iloc[:,24]\n",
    "\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(34, input_dim=23, kernel_initializer= 'normal' , activation= 'relu' ))\n",
    "model.add(Dense(1, kernel_initializer= 'normal' , activation= 'sigmoid' ))\n",
    "# Compile model\n",
    "sgd = SGD(lr=0.0, momentum=0.9, decay=0.0, nesterov=False)\n",
    "model.compile(loss= 'binary_crossentropy' , optimizer=sgd, metrics=[ 'accuracy' ])\n",
    "# learning schedule callback\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "callbacks_list = [lrate]\n",
    "# Fit the model\n",
    "model.fit(X, Y, validation_split=0.33, epochs=50, batch_size=28, callbacks=callbacks_list,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('file1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiit/anaconda3/envs/tf_training/lib/python3.7/site-packages/pandas/core/ops.py:1649: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = method(y)\n"
     ]
    }
   ],
   "source": [
    "#drop all invalid number\n",
    "data.drop(data[data['number'] == ' None'].index, axis=0, inplace=True)\n",
    "#convert obj to numeric\n",
    "data['number'] = pd.to_numeric(data['number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_training",
   "language": "python",
   "name": "tf_training"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
